"""
    模型量化

        模型的压缩方法, 使用低精度数据类型来对模型的权重或激活值进行表示, 即将高精度的模型参数通过函数映射转换关系, 进而使用低精度的数据类型来表示高精度的数据类型

    量化的优点

        降低模型加载的内存, FP32 > FP16 > INT8, 分别代表32位(bit)的单精度、16位(bit)的半精度、8位的整型, 内存依次降低
    
        量化不会带来模型训练或推理计算的加快, 反而会降低计算速度, 因为它涉及到将量化的数据反量化为原数据类型的过程
    
    
    如何进行量化

        1. 将原始数据量化为INT8为例

            原始数据: x = [1.52, 2.64, -3.45, 4.32]

            量化过程:

                    INT8的最大值为 INT_MAX =  127

                    计算缩放因子: scale_factor = 127 / absmax(x) = 127 / 4.32

                    量化后的x:   q_x = round([1.52, 2.64, -3.45, 4.32]) * scale_factor = [45, 78, -101, 127]

                    此时x量化后的值 q_x 的数全是INT8能够表示的范围内, [-128, 127]

            反量化过程:

                    x` = q_x / scale_factor = [1.53, 2.61, -3.44, 4.32]     

            注意:

                    经过反量化后发现, 反量化的结果和原始结果具有误差, 即量化误差, 所以量化操作是找到量化误差尽可能小的量化方法

                    
        2. 量化的离群值

            原始数据: x = [1.42, 1.51, 1.54, 45.3]

            量化过程:

                    absmax(x) = 45.3

                    sale_factor = 127 / absmax(x) = 2.8

                    q_x = round(x * sale_factor)  = [4, 4, 4, 127]

            反量化:

                    x` = q_x / sale_factor = [1.43, 1.43, 1.43, 45.36]
            
            发现:

                如果原始数据中存在某个或某几个值远大于其他值(即离群值), 而且其他值相差微小, 此时将它们量化之后, 再反量化回来, 会发现那些相差微小的值都变成了同一个值
                正式因为离群值的影响导致这些现象的出现, 这些现象会导致模型训练或推理时, 导致模型的效果极差  

                并且INT4量化即使没有离群值也会发生上述现象, 因为INT4所能表示的范围更小了, 那么它的所能量化的区间粒度更加粗, 则让更多元素在同一个区间的可能性就越大
                

            解决办法: 使用混合精度进行微调训练

        
        3. 混合精度分解量化

            即将针对异常的离群值, 进行检测, 对离群值所在的向量使用原精度(不进行量化), 对其他正常值使用量化, 因此称之为混合精度量化
            注意是含有大于某个阈值的离群值所在的整个向量使用原始精度(FP16/FP32), 不含大于某个阈值的正常值所在的向量进行INT8量化

            反量化的非离群值的矩阵计算结果和离群值的矩阵相加就得到了与原始数据一样的FP16的结果

        

            

    

"""