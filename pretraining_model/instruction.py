"""
    预训练模型分类

        掩码语言模型: 将一些位置的token使用MASK进行随机掩码, 让LLM学习预测这些被掩码的token, 最典型的Bert模型
                     仅计算掩码部分得loss
        
        因果语言模型: 即自回归模型, Decoder-only的生成式模型, 即基于前面得token预测下一个token, 需要有开始特殊标记和结束特殊标记EOS

        序列到序列模型: 即输入是文本, 输出也是文本, 任务较为多样化, 如掩码生成、片段回归、乱序修正
                       采用编码器和解码器得方式训练, 最典型的就是T5, 仅使用解码器的loss作为LLM的训练loss

        前缀语言模型(prefix LM): 
    
"""