"""
    lora思想    

        任何与训练模型(基座模型)存在一个维度极小(秩极小)的权重矩阵, 它是整个预训练模型最核心的权重矩阵
        lora的思想就是通过微调这个极小矩阵来实现适配下游任务, 从而可以做到通过微调极少部分权重矩阵, 又不去削弱原有的其他矩阵所表现出的能力, 还能做到适配下游任务


    lora做法

        W' = W + ΔW   W为基座模型的权重矩阵, ΔW为lora微调的低秩矩阵, W' 为微调训练后的矩阵

        ΔW = B * A   其中A和B都是秩远小于W的矩阵, 它们也是lora微调时真正参与模型训练和学习的参数, 基座模型W参数是会冻结住的, 不会参与训练

        ΔW在微调训练中不断更新优化, 训练完毕后, 它会和基座模型进行合并得到  W'


    
    





"""