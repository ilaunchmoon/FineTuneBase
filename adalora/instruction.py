"""
    adalora思想

        在显存预算确定的前提下, 针对基座模型中不同模块中的不同权重矩阵对下游任务的不同重要性, 根据重要性的不同动态分配给不同权重参数不同的低秩矩阵进行微调训练
        避免香lora那样将基座模型中的不同权重矩阵都使用一样秩为r的可学习权重矩阵去微调学习

    实现办法

        使用矩阵奇异值分解(SVD)的方法将微调训练的低秩矩阵 ΔW 分解为 P Λ Q 三个矩阵, 结合重要性得分来实现动态调整对下游任务具有不同重要程度的P和Q设置不同秩

    
    重要性得分度量方法

        直接使用 Λ 的非0元素来作为重要性得分

        使用 可学习参数 w 和 可学习参数的梯度乘积 的模  作为重要性得分

        使用灵敏度平滑技术来计算不同模块的不同重要性得分计算

    
    Loss计算

        Loss具有两部分, 一部分是模型预测结果和标签之间的损失, 一部分是PQ为正交矩阵的惩罚, 如果PQ不是正交矩阵, 则惩罚值越大

        目的: 微调训练就是让模型向着正确预测和向着P和Q为正交矩阵的方向去优化

    
        
    关于AdaLoraConfig()参数说明

            task_type ( TaskType, 可选):
                ​​功能​​: 指定任务类型 
                ​​目的​​: 帮助PEFT库针对特定任务 ( 如因果语言建模、序列分类等 ) 正确配置模型 
                ​​设置​​: 例如: TaskType.CAUSAL_LM、TaskType.SEQ_CLS等 根据你的任务选择合适的TaskType枚举值 

            target_modules ( Union[List[str], str], 必须设置):
                ​​功能​​: 要应用AdaLora的模块名称 
                ​​目的​​: 指定在模型的哪些模块上添加AdaLora层 通常这些模块是线性层 
                ​​设置​​: 可以是模块名的字符串列表, 如: ["q_proj", "v_proj"] 也可以使用正则表达式字符串 如果不知道, 可以设置为None, 但这样可能会对所有线性层应用, 导致参数过多 

            init_r ( int, 默认为12):
                ​​功能​​: 初始的秩 ( rank ) 值 
                ​​目的​​: 在训练开始前, 每个AdaLora层的初始秩 通常设置得比目标秩大一些, 以便在训练过程中有修剪空间 
                ​​设置​​: 可以设为12, 根据模型大小调整, 大模型可以更大一些 ( 如16 ) , 小模型可以小一些 ( 如6 )  

                小模型 (100M-1B): 4-8
                中等模型 (1B-7B): 8-12
                大模型 (>7B): 12-16
                作为动态调整的起点，较高的初始秩提供更大的表达容量


            target_r ( int, 默认为8):
                ​​功能​​: 最终的目标秩 
                ​​目的​​: 训练结束后, 每个AdaLora层将达到的目标秩 ( 实际上由于动态调整, 各层最终秩可能略有不同 )  
                ​​设置​​: 通常小于init_r, 例如8 可以根据需要调整, 比如为了更低的参数, 可以设置更小的值 ( 如4 )  
                建议设置总是小于init_r的值-通常为init_r的2/3


            beta1 ( float, 默认为0.85):
                ​​功能​​: 用于控制重要性分数累积的衰减率 
                ​​目的​​: 在更新重要性分数的指数移动平均 ( EMA ) 时, 作为平滑参数 ( 类似于Adam的beta1 )  它影响重要性估计的稳定性 
                ​​设置​​: 通常设置为0.85, 也可以尝试0.8到0.9之间的值 
                使用指数移动平均平滑梯度信息, beta1越大表示越依赖最近的梯度信息


            beta2 ( float, 默认为0.85):
                ​​功能​​: 与beta1类似, 但用于控制重要性分数平方的累积 
                ​​目的​​: 也是用于EMA, 对梯度信息进行平滑 
                ​​设置​​: 通常和beta1相同, 设置为0.85 
                与beta1共同作用, beta2值越大表示重要性度量变化越平滑


            tinit ( int, 默认为0):
                ​功能​​: 无修剪 ( no pruning ) 的初始训练步数 
                ​​目的​​: 在训练初期, 不进行秩的修剪, 允许模型先学习一段时间, 以便更准确地评估参数的重要性 
                ​​设置​​: 例如200, 表示前200步不修剪 
                小模型: 50-200步, 大模型: 200-500步, 前期不进行任何修剪，确保基本特征学习稳定


            tfinal ( int, 默认为0):
                ​​功能​​: 达到目标秩的时间步 ( 训练步数 )  
                ​​目的​​: 在tfinal步时, 模型的秩将下降到target_r 之后, 秩保持固定 
                ​​设置​​: 设置为一个较大的值, 如1000 ( 根据总训练步数来定 )  如果训练步数很多, 可以设置为总步数的一半或三分之二 
                设为目标总训练步数的60-80%, 线性计划从init_r减到target_r的时间框架

            deltaT ( int, 默认为1):
                ​​功能​​: 进行修剪的时间间隔 ( 步数 )  
                ​​目的​​: 每隔deltaT步, 对模型各层的秩进行一次调整 ( 修剪 )  
                ​​设置​​: 例如10, 表示每10步进行一次修剪 不宜过小 ( 计算开销大 ) , 也不宜过大 ( 调整不频繁 )  通常在10到50之间 
                10-50 (太频繁增加计算开销，太稀疏降低调整精度)
                每deltaT步进行一次秩调整(减少低重要性参数)


            lora_alpha ( int, 默认为None):
                ​​功能​​: LoRA层中的缩放因子 
                ​​目的​​: 在应用LoRA时, 将低秩矩阵的输出缩放 ( 除以rank后乘以lora_alpha ) , 相当于控制学习率大小 
                ​​设置​​: 如果为None, 则使用默认值 ( 通常为r的2倍 )  一般情况下设置为8、16或32等 可以视为与学习率有关的一个超参 



            lora_dropout ( float, 默认为0.0):
                ​​功能​​: LoRA层的dropout率 
                ​​目的​​: 防止过拟合, 随机丢弃一部分神经元 
                ​​设置​​: 0到1之间, 比如0.1 可以不用设置 ( 0 ) 或者小值 



            modules_to_save ( List[str], 可选):
                ​​功能​​: 除了LoRA层外, 还需要训练 ( 保存 ) 的模块 
                ​​目的​​: 对于某些模块 ( 如词嵌入层或输出层 ) , 有时需要完全微调, 这些模块将被指定并训练 
                ​​设置​​: 例如: ["word_embeddings"] 如果不需要额外训练其他模块, 则为None 



            bias ( str, 默认为"none"):
                ​​功能​​: 是否训练偏置项 
                ​​目的​​: 控制是否在训练中更新偏置参数 
                ​​设置​​: 可选值有"none" ( 不训练偏置 ) 、"all" ( 训练所有偏置 ) 或"lora_only" ( 仅训练LoRA层的偏置 )  



            layers_to_transform ( Union[List[int], int], 可选):
                ​​功能​​: 指定要转换的层的索引 
                ​​目的​​: 仅对指定层应用AdaLora 如果为None, 则适用于所有层 
                ​​设置​​: 例如: [0, 1, 2] 或 0 ( 仅一层 )  如果为None, 则所有层都被转换 


            layers_pattern ( str, 可选):
                ​​功能​​: 用于匹配要转换层的模式 ( 正则表达式 )  
                ​​目的​​: 与target_modules一起, 用于识别模型中哪些层要添加AdaLora 
                ​​设置​​: 通常不需要指定, 除非有特殊模块结构 



            rank_pattern ( dict, 默认为{}):
                ​​功能​​: 指定不同层的秩模式 ( 即不同层可以有不同的秩 )  
                ​​目的​​: 允许手动设置某些层具有特定的秩 ( 而非统一设置 )  
                ​​设置​​: 例如: {"attention.": 8, "mlp.": 4} 默认空字典表示所有层使用相同的秩 ( init_r和target_r )  


            alora_layers ( dict, 可选):
                功能​​: 内部使用, 通常不需要设置 



            inference_mode ( bool, 默认为False):
                ​​功能​​: 是否在推理模式下 
                ​​目的​​: 如果为True, 则加载的模型将用于推理 ( 不参与训练 )  通常在加载微调后的模型时使用 
                ​​设置​​: 训练时为False ( 默认 )  



"""