"""

    讨论在模型训练过程, 无论是微调训练、SFT训练, 如何降低模型显存的占用

    方法

        模型参数不变的情况下, 降低模型每个参数占用的字节数(内存)的方法来降低模型训练过程的显存占用
    
    
    如何降低模型训练过程中的参数的占用显存

        模型参数默认是FP32, FP32占32比特(4字节), 只需使用更低精度的数据类型来表示FP32的数据类型即可

        一般使用半精度的数据类型来表示:  如fp16(半精度)、或bfloat16、   或int8、      或fp4、或nf4
                                    占16Bit(2字节)、 16Bit(2字节)  8Bit(1字节)   4Bit(0.5字节)


    半精度

        半精度是指的FP16, 为一种浮点数格式, 它使用16bit表示一个浮点数(2字节), 即它的指数位和尾数位的位数都减半, 符号位还是1位

    
    半精度优点

        模型微调训练过程中, 启用半精度训练能够有效的节约显存, 并提升计算速度

        
    半精度缺点

        模型训练中, 计算可能会发生溢出和舍入的问题, 会导致无法表示出更加精确的数值无法表示出来, 此时可以考虑使用BF16来替代


    
        
    如何启动半精度训练

        1. 模型加载之后使用model.half()的方式将单精度转为半精度

            model = AutoModel.from_pretrained()

            model = model.half()
        
            
        2. 模型加载时就使用半精度加载, 即使用torch_dtype=torch.half参数

            model = AutoModel.from_pretrained(model_name/model_dir, torch_dtype=torch.half)

        
        注意: 推荐使用第二种方式, 因为第一种方法加载还是使用单精度加载, 然后再转为半精度, 还是使用了单精度加载

        
"""